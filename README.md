# âœ¨VISÃƒO GERAL DO PROJETO
O projeto tenta otimizar um dos processos de RH que Ã© o *matching* de candidatos para a empresa [Decision](https://webrh.decisionbr.com.br/).

A dor a ser resolvida nesse projeto consiste em fazer um MVP que seja capaz de responder Ã  seguinte pergunta: dado uma vaga especÃ­fica, quais seriam os melhores candidatos para essa vaga considerando que eu tenho o texto dos currÃ­culos dos candidatos armazenados no sistema, utilizando-se de soluÃ§Ãµes de inteligÃªncia artificial.

Para isso, foram utilizadas tÃ©cnicas de *embeddings* e similaridade do cosseno que serÃ£o detalhadas ao longo deste documento.  

## ðŸŽ¯Objetivo
O projeto em questÃ£o Ã© um MVP para achar os melhores candidatos para uma determinada vaga dado que temos os textos dos currÃ­culos dos candidatos. Deve ser possÃ­vel escolher quais sÃ£o os melhores K candidatos para a vaga. 

## ðŸ’¡SoluÃ§Ã£o Proposta
A soluÃ§Ã£o proposta consiste no seguinte:
* Utilizar a API do gemini para gerar *embeddings* tanto da vaga requerida como dos currÃ­culos dos candidatos cadastrados.
* Fazer a similaridade do cosseno entre a vaga especÃ­fica (a vaga que o usuÃ¡rio quer) e todos os candidatos cadastrados no banco vetorial.
* Retornar os K candidatos mais similares Ã  vaga, de modo que o usuÃ¡rio pode escolher o valor de K. 
* Por fim, logar estatÃ­sticas do modelo utilizando mlflow para poder monitorar o desempenho do modelo e possÃ­veis drifts que possam acontecer em produÃ§Ã£o ao longo do tempo.

## Stack TecnolÃ³gica:
Foram utilizados as seguintes tecnologias para construir esse projeto:
* **Python3 (versÃ£o 3.12)**: linguagem de programaÃ§Ã£o utilizada.
* **poetry**: gerenciador de pacotes do python. 
* **Docker e Docker compose**: conteinerizaÃ§Ã£o e orquestraÃ§Ã£o dos serviÃ§os (API, ChromaDB, Redis e, MLflow).
* **ChromaDB**: banco vetorial para armazenar e consultar embeddings semÃ¢nticos.
* **Redis**: armazenamento chave-valor em memÃ³ria, de alta performance.
* **Mlflow**: rastreamento de experimentos (parÃ¢metros, mÃ©tricas e artefatos).
* **Flask**: microframework web para a API.
* **Gunicorn**: servidor WSGI para produÃ§Ã£o.
* **Geimini-API**: API do modelo LLM.



# ESTRUTURA DO PROJETO
```
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ config.py
â”œâ”€â”€ database
â”‚Â Â  â”œâ”€â”€ READ.ME.txt
â”‚Â Â  â”œâ”€â”€ applicants.json
â”‚Â Â  â”œâ”€â”€ candidates_dim3072.jsonl
â”‚Â Â  â”œâ”€â”€ prospects.json
â”‚Â Â  â””â”€â”€ vagas.json
â”œâ”€â”€ docker
â”‚Â Â  â””â”€â”€ entrypoint.sh
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ poetry.lock
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ run.py
â”œâ”€â”€ scripts
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ export_data.py
â”‚Â Â  â”œâ”€â”€ generate_chromadb_file.py
â”‚Â Â  â”œâ”€â”€ generate_embeddings.py
â”‚Â Â  â””â”€â”€ import_data.py
â”œâ”€â”€ src
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ app.py
â”‚Â Â  â”œâ”€â”€ extensions.py
â”‚Â Â  â”œâ”€â”€ models.py
â”‚Â Â  â”œâ”€â”€ routes.py
â”‚Â Â  â”œâ”€â”€ services
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Data.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ chromadb_info.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ gemini_api.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ log.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ retrieve_data.py
â”‚Â Â  â”œâ”€â”€ static
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ css
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ style.css
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ img
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ decision_logo.png
â”‚Â Â  â”‚Â Â  â””â”€â”€ js
â”‚Â Â  â”‚Â Â      â””â”€â”€ script.js
â”‚Â Â  â””â”€â”€ templates
â”‚Â Â      â””â”€â”€ index.html
â”œâ”€â”€ .env.sample
â””â”€â”€ wsgi.py
```

Explicando os arquivos que estÃ£o na raiz do projeto:
* **Dockerfile:** arquivo de configuraÃ§Ã£o do Docker, foi utilizado para gerar um container da API do Flask
* **LICENSE**: arquivo de licensa do projeto (licenciado sob a MIT License).
* **README.md**: este arquivo, com instruÃ§Ãµes e documentaÃ§Ã£o do projeto.
* **config.py**: arquivo usado no padrÃ£o Application Factory do Flask, centraliza as configuraÃ§Ãµes para *development*, *test* e *production* (geralmente lendo variÃ¡veis do ambiente/.env).
* **docker-compose.yml**: arquivo de configuraÃ§Ã£o do Docker compose, Ã© responsÃ¡vel por declarar os serviÃ§os do sistema bem como configurar o ambiente de cada um deles.
* **poetry.lock**: arquivo de lock do Poetry, tem a responsabilidade de armazenar e travar as versÃµes de cada dependÃªncia na Ã¡rvore de dependÃªncias do projeto para garantir a reprodutibilidade.
* **pyproject.toml**: arquivo de configuraÃ§Ã£o do Poetry e metadados do projeto, Ã© reponsÃ¡vel por armazenar as bibliotecas utilizadas no projeto bem como algumas informaÃ§Ãµes gerais.
* **run.py**: arquivo gerado pelo padrÃ£o factory do Flask, Ã© utilizado quando se quer executar a aplicaÃ§Ã£o em modo development utilizando o comando "poetry run python3 run.py", nÃ£o Ã© utilizado em produÃ§Ã£o.
* **wsgi.py**: arquivo que inicia a aplicaÃ§Ã£o em produÃ§Ã£o. Ã‰ utilizado pelo servidor web *gunicorn* para iniciar o servidor python da API, ele contÃ©m tudo que Ã© preciso para criar e executar a aplicaÃ§Ã£o.
* **.env.sample**: arquivo que possui uma amostra das variÃ¡veis de ambiente do projeto, basicamente possui o valor de todas as variÃ¡veis de ambiente menos a GEMINI_API_KEY, por questÃµes de seguranÃ§a. O arquivo .env deve ser uma cÃ³pia desse arquivo, sÃ³ que nele vocÃª deve preencher o valor de GEMINI_API_KEY.

# INSTRUÃ‡Ã•ES DE DEPLOY LOCAL
Para executar o projeto local, vocÃª precisa ter o docker e o docker compose instalado na sua mÃ¡quina e entÃ£o executar os seguintes passos:
* Baixe o projeto com o seguinte comando:
```bash
git clone https://github.com/valteci/Datathon.git
```
* Entre no diretÃ³rio do projeto com o comando:
```bash
cd Datathon
```
* Crie uma pasta chamada "database" na raiz do projeto. A versÃ£o entregue aos professores jÃ¡ possui essa pasta, nÃ£o precisa criÃ¡-la caso seja o professor.
* Baixe [esse arquivo](https://drive.google.com/file/d/16TV4tOEU45j0Uq457uU2JIp8vEx9lHuv/view?usp=sharing) e coloque-o dentro da pasta database. A versÃ£o entregue aos professores jÃ¡ possui esse arquivo, nÃ£o precisa baixÃ¡-lo caso seja o professor.
* Criar o arquivo ".env" na raiz do projeto, ele Ã© uma cÃ³pia do arquivo .env.sample que jÃ¡ vem no projeto, mas vocÃª precisa preencher a variÃ¡vel de ambiente GEMINI_API_KEY com uma chave vÃ¡lida dentro do arquivo .env. A versÃ£o entregue aos professores jÃ¡ possui um .env vÃ¡lido, nÃ£o precisa criÃ¡-lo caso seja um professor.
* Na raiz do projeto, executar o seguinte comando para rodar a aplicaÃ§Ã£o:
```bash
docker compose up --build
```
* Monitorar a saÃ­da do comando no passo anterior. Quando os serviÃ§os subirem, o servidor vai precisar fazer o import dos embeddings dos candidatos, o que vai demorar de 1 a 2 minutos na primeira vez que o sistema for executado. A aplicaÃ§Ã£o estarÃ¡ pronta para uso quando os imports forem concluÃ­dos (done) e aparecer uma mensagem parecida com essa "[INFO] Booting worker with pid..."
* Acesse a aplicaÃ§Ã£o por meio do endereÃ§o [localhost:5000](http://localhost:5000)


# DEPLOY NO GOOGLE CLOUD
Essa projeto foi implantado do google cloud e pode ser acessado pelo seguinte link: [http://34.39.160.178:5000/](http://34.39.160.178:5000/)


# ROTAS E EXEMPLOS DE CHAMADAS A API

# FLUXO DE PROCESSAMENTO

